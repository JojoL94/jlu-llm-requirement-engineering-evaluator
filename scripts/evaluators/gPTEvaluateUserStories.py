import os
import csv
import time
import datetime
import openai
import backoff
from dotenv import load_dotenv
from collections import deque

# Lade die Umgebungsvariablen aus der .env Datei
load_dotenv()

# Setze den OpenAI API-Schlüssel
api_key = os.getenv('OPENAI_API_KEY')
if api_key is None:
    raise ValueError("OPENAI_API_KEY not set in environment")

client = openai.OpenAI(api_key=api_key)

# Kriterienliste nach Lucassen et al. (2016)
criteria = [
    "Atomic: A user story expresses a requirement for exactly one feature.",
    "Minimal: A user story contains nothing more than role, means and ends",
    "Well-formed: A user story includes at least a role and a means.",
    "Conflict-free: A user story should not be inconsistent with any other user story.",
    "Conceptually sound: The means expresses a feature and the ends expresses a rationale, not something else.",
    "Problem-oriented: A user story only specifies the problem, not the solution to it.",
    "Unambiguous: A user story avoids terms or abstractions that may lead to multiple interpretations.",
    "Complete: Implementing a set of user stories creates a feature-complete application, no steps are missing.",
    "Explicit dependencies: Link all unavoidable, non-obvious dependencies on user stories.",
    "Full sentence: A user story is a well-formed full sentence.",
    "Independent: The user story is self-contained, avoiding inherent dependencies on other user stories.",
    "Scalable: User stories do not denote too coarse-grained requirements that are difficult to plan and prioritize.",
    "Uniform: All user stories follow roughly the same template.",
    "Unique: Every user story is unique, duplicates are avoided."
]

evaluate_prompt_template = """
### Prompt:

You are a skilled software developer responsible for evaluating the quality of user stories. Evaluate the following user stories based on the given criterion, and explain your thought process step by step.

Use Case Context:
------Start of Use Case------
{use_case}
-------End of Use Case-------

Reference User Stories generated by gpt-3.5-turbo:
------Start of Reference User Stories------
{reference_user_stories}
-------End of Reference User Stories-------

User Stories to Evaluate:
------Start of User Stories------
{user_stories}
-------End of User Stories-------

Criterion: 
------Start of criterion------
{criterion}
-------End of criterion-------

Evaluate each user story individually within the context of the entire use case and the set of user stories. Provide a Yes or No response for each criterion and a short explanation if necessary of your reasoning for each response. Use the following format and stay in this format:
------Start of format------
Criterion: [Name of Criterion]
User Story [Number of User Story]: [Yes/No] - [Explanation if necessary and thought process if necessary]
User Story [Number of User Story]: [Yes/No] - [Explanation if necessary and thought process if necessary]
User Story [Number of User Story]: [Yes/No] - [Explanation if necessary and thought process if necessary]
(...rest of user stories with their evaluation in the same format as before)

Overall Result: [Yes/No]
Explanation: [Short Explanation of the overall result if necessary. Max 3 sentences]
End of Evaluation
-------End of format-------

Scores:
Yes - Criterion met
No - Criterion not met or partially met

Please evaluate the user stories based on the given criterion. Provide the evaluation for each user story in the same format as described and explain your thought process if necessary. Very Important: If you are finished with the format, write "End of Evaluation", as described and stay in the described Format.
"""

# Rate limiting
request_queue = deque()
token_usage = 0
REQUESTS_PER_MINUTE = 3500
REQUESTS_PER_DAY = 10000
TOKENS_PER_MINUTE = 60000
DAILY_REQUEST_LIMIT = 10000
DAILY_TOKEN_LIMIT = 200000

def rate_limit_check():
    global token_usage
    current_time = datetime.datetime.now()

    # Entfernen von Anfragen, die älter als eine Minute sind, aus der Warteschlange
    while request_queue and (current_time - request_queue[0]).total_seconds() > 60:
        request_queue.popleft()

    # Überprüfen, ob die Anzahl der Anfragen in der letzten Minute das Limit überschreitet
    if len(request_queue) >= REQUESTS_PER_MINUTE:
        time_to_wait = 60 - (current_time - request_queue[0]).total_seconds()
        print(f"Rate limit reached. Waiting for {time_to_wait} seconds.")
        time.sleep(time_to_wait)
        rate_limit_check()  # Nach dem Warten erneut prüfen

    # Überprüfen, ob die Anzahl der verwendeten Tokens in der letzten Minute das Limit überschreitet
    if token_usage >= TOKENS_PER_MINUTE:
        print("Token limit reached. Waiting for 60 seconds.")
        time.sleep(60)
        token_usage = 0  # Token-Nutzung nach dem Warten zurücksetzen

def check_daily_limits():
    if len(request_queue) >= DAILY_REQUEST_LIMIT:
        print("Daily request limit exceeded. Exiting.")
        exit()
    if token_usage >= DAILY_TOKEN_LIMIT:
        print("Daily token limit exceeded. Exiting.")
        exit()

@backoff.on_exception(backoff.expo, openai.RateLimitError)
def chat_completions_with_backoff(**kwargs):
    return client.chat.completions.create(**kwargs)

def start_evaluation(use_case, reference_user_stories, user_stories, criterion):
    global token_usage
    print(f"Starting evaluation with criterion: {criterion}...")
    start_time = time.time()
    rate_limit_check()
    check_daily_limits()
    response = chat_completions_with_backoff(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that evaluates software development user stories."},
            {"role": "user", "content": evaluate_prompt_template.format(
                use_case=use_case,
                reference_user_stories=reference_user_stories,
                user_stories=user_stories,
                criterion=criterion
            )}
        ],
        max_tokens=1500,
        temperature=0.2
    )
    end_time = time.time()
    print(f"Evaluation complete. Time taken: {end_time - start_time} seconds")
    token_usage += response.usage.total_tokens
    request_queue.append(datetime.datetime.now())
    return response.choices[0].message.content

def main():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    input_use_cases_file = os.path.join(current_dir, '../../data/generated', "generated_use_cases.csv")
    input_user_stories_file = os.path.join(current_dir, '../../data/generated', "generated_user_stories.csv")
    input_reference_user_stories_file = os.path.join(current_dir, '../../data/generated', "generated_reference_user_stories.csv")

    data_dir = os.path.join(current_dir, '../../data/evaluated')
    os.makedirs(data_dir, exist_ok=True)
    output_file = os.path.join(data_dir, "evaluated_by_GPT-3.5-Turbo_user_stories.csv")

    use_cases_dict = {}
    with open(input_use_cases_file, mode='r', encoding='utf-8') as file:
        reader = csv.reader(file)
        headers = next(reader)
        for row in reader:
            industry, title, description, actor, preconditions, trigger, use_case = row
            use_cases_dict[title] = use_case

    reference_user_stories_dict = {}
    with open(input_reference_user_stories_file, mode='r', encoding='utf-8') as file:
        reader = csv.reader(file)
        headers = next(reader)
        for row in reader:
            industry, title, model, reference_user_stories = row
            reference_user_stories_dict[(industry, title)] = reference_user_stories

    with open(input_user_stories_file, mode='r', encoding='utf-8') as file:
        reader = csv.reader(file)
        headers = next(reader)

        if len(headers) < 4:
            raise ValueError("The CSV file does not have the expected number of columns.")

        rows = [row for row in reader if len(row[3]) > 10]  # Skip rows with user stories shorter than 10 characters

    evaluations = []

    for idx, row in enumerate(rows):
        if len(row) < 4:
            print(f"Warning: Row does not have enough values: {row}")
            continue

        industry, title, model, user_stories = row
        use_case = use_cases_dict.get(title, "No use case found for this title")
        reference_user_stories = reference_user_stories_dict.get((industry, title), "No reference user stories found for this use case")
        print(f"Evaluating user stories for {title} in {industry} (Model: {model})... ({idx + 1}/{len(rows)})")

        full_evaluation = ""  # Start collecting the full evaluation

        for criterion in criteria:
            try:
                evaluation = start_evaluation(use_case, reference_user_stories, user_stories, criterion)
            except ValueError as e:
                print(f"Error: {e}")
                break

            full_evaluation += "\n" + evaluation  # Append each criterion evaluation to the full evaluation
            print(f"Intermediate result after evaluating {criterion.split(':')[0]} for {title}:\n", evaluation)

        print(f"Finished evaluating {title} in {industry} (Model: {model}).")
        print("Final Evaluation result:\n", full_evaluation)
        evaluations.append([industry, title, model, user_stories, full_evaluation])  # Save the full evaluation

    with open(output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(headers + ["Evaluation"])
        writer.writerows(evaluations)

    print(f"User stories have been evaluated and saved to {output_file}")

if __name__ == "__main__":
    main()

